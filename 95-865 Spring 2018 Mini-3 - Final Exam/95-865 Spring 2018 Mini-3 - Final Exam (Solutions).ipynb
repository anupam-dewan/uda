{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 95-865 Spring 2018 Mini-3 Final Exam [100 points]\n",
    "\n",
    "In this final exam, you will compare how well two different methods do on a prediction task (solving word analogies):\n",
    "\n",
    "* The first method (which we will call **Method 1**) is based on using pre-trained word embedding vectors.\n",
    "* The second method (which we will call **Method 2**) is based on factoring a pointwise mutual information (PMI) matrix.\n",
    "\n",
    "Both of these methods produce, for each word in a dictionary, a vector representation (so both methods produce so-called \"word embeddings\"). However, it is not clear which method should do better at solving word analogies.\n",
    "\n",
    "As a remark: being able to solve word analogies is a basic task in natural language processing (and in fact, it has been used in GRE and SAT exams in the US to help gauge human English ability!).\n",
    "\n",
    "**Note:** The completed notebook consumed ~3GB of RAM, and took ~4 minutes to execute. Keep this in mind when facing issues with running time or memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1\n",
    "\n",
    "### (a) Load pre-trained word vectors [15 points]\n",
    "\n",
    "The file `pretrained_word2vec_vectors.txt` contains 100-dimensional word vectors for $N$ words, trained using the gensim package and the word2vec model. The file is formatted as:\n",
    "\n",
    "```\n",
    "no_of_words vector_dimensionality\n",
    "word vector-value vector-value ...\n",
    "...\n",
    "```\n",
    "\n",
    "   1. Read the word vectors into a numpy array `w2v` with 100 columns and $N$ rows.\n",
    "   2. Construct a list `w2v_words` of $N$ words ordered according to the row index of their vector in `w2v`.\n",
    "   3. Normalize each vector by its L2 norm (hint: `numpy.linalg.norm`). After normalization, each row of `w2v` should have an L2 norm of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "w2v_words = set([])\n",
    "w2v_to_idx = {}\n",
    "idx_to_w2v = {}\n",
    "with open(\"pretrained_word2vec_vectors.txt\", \"r\") as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx == 0:  # important: treat the header differently!\n",
    "            nwords, ndim = map(int, line.split())\n",
    "            w2v = np.zeros((nwords, ndim))  # the header tells us the shape of w2v\n",
    "        else:\n",
    "            fields = line.strip().split()\n",
    "            \n",
    "            word = fields[0]\n",
    "            w2v_words.add(word)\n",
    "            w2v_to_idx[word] = idx - 1  # subtract 1 to start counting words from 0 (ignoring the header)\n",
    "            idx_to_w2v[idx - 1] = word\n",
    "            \n",
    "            vector = list(map(float, fields[1:]))\n",
    "            w2v[idx-1, :] = vector\n",
    "\n",
    "w2v = np.array([v/np.linalg.norm(v) for v in w2v])  # normalize each vector to have unit L2 norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be used to test whether your normalization is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert np.allclose(np.array([np.linalg.norm(v) for v in w2v]),\n",
    "                   np.ones(w2v.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Evaluate how well the this method does on predicting word analogies [20 points]\n",
    "\n",
    "The `questions-words.txt` file contains word analogy tasks, one on each line. Consider the example:\n",
    "\n",
    "```\n",
    "Athens Greece Baghdad Iraq\n",
    "```\n",
    "\n",
    "This should be read as: \"Athens is to Greece, as Baghdad is to ?\". A good word embedding should predict the last word in the analogy well.\n",
    "\n",
    "Let `a b c d` be the four words in an analogy task. Let `v` be the word embedding, so `v[w]` is the vector for word `w`. The accuracy of an embedding is computed as follows:\n",
    "\n",
    "   1. Compute `pred = v[c] + (v[b] - v[a])`\n",
    "   2. Find the word `y` in the corpus with vector `v[y]` second-closest to `pred` (in terms of the cosine distance). This is because the closes word to `pred` will most likely be `c`; hence, we use the second-closest instead.\n",
    "   3. If `y` is equal to `d`, count the task as a success. If not, count it as a failure.\n",
    "\n",
    "For the word embedding you loaded in part (a) and stored in the variable `w2v`, compute the number of successes using the three steps outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successes: 38\n",
      "CPU times: user 16.6 s, sys: 286 ms, total: 16.8 s\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ntasks = 0\n",
    "w2v_success = 0\n",
    "with open(\"questions-words.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        a, b, c, d = line.strip().split()\n",
    "        \n",
    "        pred_w2v = w2v[w2v_to_idx[c]] + (w2v[w2v_to_idx[b]] - w2v[w2v_to_idx[a]])\n",
    "        \n",
    "        # interesting fact (that you didn't have to use): for vectors with unit L2 norm,\n",
    "        # the cosine *similarity* between two vectors is actually just their dot product\n",
    "        # so we just look for whichever word in `w2v` has the second largest dot product\n",
    "        # with `pred_w2v` (you could have just used the cosine distance function\n",
    "        # provided by scipy)\n",
    "        predword_w2v_idx = np.argsort(np.dot(w2v, pred_w2v))[-2]\n",
    "        predword_w2v = idx_to_w2v[predword_w2v_idx]\n",
    "        if predword_w2v == d:\n",
    "            w2v_success += 1\n",
    "\n",
    "print('Number of successes:', w2v_success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2\n",
    "\n",
    "For this second method, we will learn a word embedding from scratch using an existing text corpus, computing PMI values (and storing it into a 2D table, i.e., a matrix), and factoring the PMI matrix to obtain word embeddings. We can then use the word embeddings to, just as in part (b) for Method 1 above, predict word analogies.\n",
    "\n",
    "### (a) Load text corpus [10 points]\n",
    "\n",
    "The file `brown_corp.txt` contains the Brown University Standard Corpus of Present-Day American English: the first modern, computer-readable text corpus.\n",
    "\n",
    "The file has a single line with a list of words separated by spaces. Sentences are separated by a period with spaces on either side, like \" . \".\n",
    "\n",
    "   1. Construct a list `brown_corpus` that is a list of lists. Each element of `brown_corpus` is a sentence (as a list of words).\n",
    "   2. Discard any word not in `w2v_words`.\n",
    "   3. Print the final size of the Brown corpus vocabulary.\n",
    "   4. Print the set of words in the Brown corpus vocabulary that are not in `w2v_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line = open(\"brown_corp.txt\", \"r\").readlines()[0].strip()\n",
    "sentences = line.split(\" . \")\n",
    "brown_corpus = []\n",
    "brown_words = set([])\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    words = [w for w in words if w in w2v_words]  # only keep words that are also in `w2v_words`\n",
    "    brown_corpus.append(words)\n",
    "    for word in words:\n",
    "        brown_words.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Construct word embeddings via PMI matrix factorization [30 points]\n",
    "\n",
    "Refer to the recitation of January 26, 2018 for hints (in case you do not have your notes handy, at the end of this problem's instructions, we have links to the recitation notes).\n",
    "\n",
    "   1. Construct a matrix `PMI` where each cell contains the pointwise mutual information between a pair of words in the preprocessed Brown corpus. Bigrams are counted for any two words occurring in the same sentence. (20 points)\n",
    "   2. Factorize `PMI` using SVD (`scipy.sparse.linalg.svds`) to obtain 100-dimensional vectors for each word, stored in a matrix `U`. Normalize each vector by its L2 norm. (10 points)\n",
    "   \n",
    "The PMI value for a pair of words (x, y) is (note that the recitation has a typo in this formula): $$ \\textrm{PMI}(x,y) = \\textrm{log}(\\frac{p(x,y)}{p(x)p(y)}) $$ where $p(x,y)$ is the probability of the bigram $x y$ occurring in the corpus, and $p(x)$ is the probability of the unigram $x$ occurring in the corpus.\n",
    "\n",
    "Be careful during division: dividing two integers may result in a 0, if none of them are first converted to `float`s.\n",
    "\n",
    "**Notes from the recitation on January 26, 2018:**\n",
    "\n",
    "* PDF: http://www.eyeshalfclosed.com/teaching/95865-recitation-word2vec_as_PMI.pdf\n",
    "* Jupyter notebook: https://gist.github.com/emaadmanzoor/1d06e0751a3f7d39bc6814941b37531d\n",
    "* Dataset: https://www.kaggle.com/hacker-news/hacker-news-posts/downloads/HN_posts_year_to_Sep_26_2016.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.8 s, sys: 572 ms, total: 23.4 s\n",
      "Wall time: 24.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from scipy.sparse.linalg import svds, norm\n",
    "from scipy.sparse import csc_matrix\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "\n",
    "# unigram and bigram counts\n",
    "cx = Counter()\n",
    "cxy = Counter()\n",
    "for text in brown_corpus:\n",
    "    for x in text:\n",
    "        cx[x] += 1.\n",
    "    for x, y in map(sorted, combinations(text, 2)):\n",
    "        cxy[(x, y)] += 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sums\n",
    "sx = float(sum(cx.values()))\n",
    "sxy = float(sum(cxy.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.86 ms, sys: 2.31 ms, total: 11.2 ms\n",
      "Wall time: 13.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# unigram - vector lookup\n",
    "x2i, i2x = {}, {}\n",
    "for i, x in enumerate(cx.keys()):\n",
    "    x2i[x] = i\n",
    "    i2x[i] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.89 s, sys: 598 ms, total: 9.49 s\n",
      "Wall time: 9.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# PMI\n",
    "from math import log\n",
    "pmi_samples = Counter()\n",
    "data, rows, cols = [], [], []\n",
    "for (x, y), n in cxy.items():\n",
    "    rows.append(x2i[x])\n",
    "    cols.append(x2i[y])\n",
    "    data.append(log((n / sxy) / (cx[x] / sx) / (cx[y] / sx)))\n",
    "    pmi_samples[(x, y)] = data[-1]\n",
    "PMI = csc_matrix((data, (rows, cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 s, sys: 290 ms, total: 14.3 s\n",
      "Wall time: 7.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# SVD\n",
    "from scipy.sparse.linalg import svds\n",
    "U, _, _ = svds(PMI, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U = np.array([v/np.linalg.norm(v) for v in U])  # normalize each row to have unit L2 norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be used to test whether your normalization is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert np.allclose(np.array([np.linalg.norm(v) for v in U]),\n",
    "                   np.ones(U.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Evaluate how well the this method does on predicting word analogies [20 points]\n",
    "\n",
    "Please repeat part (b) from Method 1 except now using the word embedding stored in variable `U` computed for this second PMI-based method. Once again, print the total number of successes. **Then clearly state which method has a higher number of successes in predicting word analogies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successes: 165\n",
      "CPU times: user 14.8 s, sys: 208 ms, total: 15 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ntasks = 0\n",
    "pmi_success = 0\n",
    "w2v_success = 0\n",
    "with open(\"questions-words.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        a, b, c, d = line.strip().split()\n",
    "        \n",
    "        pred_pmi = U[x2i[c]] + (U[x2i[b]] - U[x2i[a]])\n",
    " \n",
    "        predword_pmi_idx = np.argsort(np.dot(U, pred_pmi))[-2]\n",
    "        predword_pmi = i2x[predword_pmi_idx]\n",
    "        if predword_pmi == d:\n",
    "            pmi_success += 1\n",
    "        \n",
    "print('Number of successes:', pmi_success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer for which method does better** (just say either Method 1 or Method 2): Method 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Briefly explain why we cannot easily do k-fold cross validation with either of the methods here. [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** In the first method, we do not know what the training data are that was used for training the model, and as a result have no way of repeating this training procedure for different folds of the data. In the second method, we do not have any sort of ground truth word analogies associated with different parts of the Brown University text corpus. In order to do k-fold cross validation, we would need to be able to split up the training data into folds, where each fold has both training and validation sets. It's not clear how one would set up the validation sets in this case!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
