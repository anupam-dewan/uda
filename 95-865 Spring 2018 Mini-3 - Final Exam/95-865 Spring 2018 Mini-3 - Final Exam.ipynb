{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 95-865 Spring 2018 Mini-3 Final Exam [100 points]\n",
    "\n",
    "In this final exam, you will compare how well two different methods do on a prediction task (solving word analogies):\n",
    "\n",
    "* The first method (which we will call **Method 1**) is based on using pre-trained word embedding vectors.\n",
    "* The second method (which we will call **Method 2**) is based on factoring a pointwise mutual information (PMI) matrix.\n",
    "\n",
    "Both of these methods produce, for each word in a dictionary, a vector representation (so both methods produce so-called \"word embeddings\"). However, it is not clear which method should do better at solving word analogies.\n",
    "\n",
    "As a remark: being able to solve word analogies is a basic task in natural language processing (and in fact, it has been used in GRE and SAT exams in the US to help gauge human English ability!).\n",
    "\n",
    "**Note:** The completed notebook consumed ~3GB of RAM, and took ~4 minutes to execute. Keep this in mind when facing issues with running time or memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1\n",
    "\n",
    "### (a) Load pre-trained word vectors [15 points]\n",
    "\n",
    "The file `pretrained_word2vec_vectors.txt` contains 100-dimensional word vectors for $N$ words, trained using the gensim package and the word2vec model. The file is formatted as:\n",
    "\n",
    "```\n",
    "no_of_words vector_dimensionality\n",
    "word vector-value vector-value ...\n",
    "...\n",
    "```\n",
    "\n",
    "   1. Read the word vectors into a numpy array `w2v` with 100 columns and $N$ rows.\n",
    "   2. Construct a list `w2v_words` of $N$ words ordered according to the row index of their vector in `w2v`.\n",
    "   3. Normalize each vector by its L2 norm (hint: `numpy.linalg.norm`). After normalization, each row of `w2v` should have an L2 norm of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "w2v_words=[]\n",
    "with open('pretrained_word2vec_vectors.txt','r') as f:\n",
    "    first_line =f.readline().strip()\n",
    "    nrows,ncols =map(int,first_line.split(\" \"))\n",
    "    w2v=np.zeros((nrows,ncols))\n",
    "    for indx,line in enumerate(f):\n",
    "        fields=line.strip().split(\" \")\n",
    "        word=fields[0]\n",
    "        vector=[float(x) for x in fields[1:]]\n",
    "        #1.\n",
    "        w2v[indx,:]=vector\n",
    "        #2.\n",
    "        w2v_words.append(word)\n",
    "norms =np.linalg.norm(w2v,axis=1)\n",
    "for row in range(w2v.shape[0]):\n",
    "    w2v[row,:]/=norms[row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be used to test whether your normalization is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert np.allclose(np.array([np.linalg.norm(v) for v in w2v]),\n",
    "                   np.ones(w2v.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Evaluate how well the this method does on predicting word analogies [20 points]\n",
    "\n",
    "The `questions-words.txt` file contains word analogy tasks, one on each line. Consider the example:\n",
    "\n",
    "```\n",
    "Athens Greece Baghdad Iraq\n",
    "```\n",
    "\n",
    "This should be read as: \"Athens is to Greece, as Baghdad is to ?\". A good word embedding should predict the last word in the analogy well.\n",
    "\n",
    "Let `a b c d` be the four words in an analogy task. Let `v` be the word embedding, so `v[w]` is the vector for word `w`. The accuracy of an embedding is computed as follows:\n",
    "\n",
    "   1. Compute `pred = v[c] + (v[b] - v[a])`\n",
    "   2. Find the word `y` in the corpus with vector `v[y]` second-closest to `pred` (in terms of the cosine distance). This is because the closes word to `pred` will most likely be `c`; hence, we use the second-closest instead.\n",
    "   3. If `y` is equal to `d`, count the task as a success. If not, count it as a failure.\n",
    "\n",
    "For the word embedding you loaded in part (a) and stored in the variable `w2v`, compute the number of successes using the three steps outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-8f0453a0c660>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-8f0453a0c660>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    a,b,c,d =line..strip().split(\" \")\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distsance import cosine\n",
    "with opne('questions-answer.txt','r') as f:\n",
    "    for line in f:\n",
    "        a,b,c,d =line.strip().split(\" \")\n",
    "        aidx,bidx,cidx,didx = wored_to_indx[a],wored_to_indx[b],wored_to_indx[c],wored_to_indx[d]\n",
    "        pred = w2v[cidx]+(w2v[bidx]-w2v[aidx])\n",
    "        dists = cdists(pred,w2v,metric=\"cosine\")\n",
    "        sorted_indices=np.argsort(dists)\n",
    "        second_largest_indx = sorted_indices[1]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2\n",
    "\n",
    "For this second method, we will learn a word embedding from scratch using an existing text corpus, computing PMI values (and storing it into a 2D table, i.e., a matrix), and factoring the PMI matrix to obtain word embeddings. We can then use the word embeddings to, just as in part (b) for Method 1 above, predict word analogies.\n",
    "\n",
    "### (a) Load text corpus [10 points]\n",
    "\n",
    "The file `brown_corp.txt` contains the Brown University Standard Corpus of Present-Day American English: the first modern, computer-readable text corpus.\n",
    "\n",
    "The file has a single line with a list of words separated by spaces. Sentences are separated by a period with spaces on either side, like \" . \".\n",
    "\n",
    "   1. Construct a list `brown_corpus` that is a list of lists. Each element of `brown_corpus` is a sentence (as a list of words).\n",
    "   2. Discard any word not in `w2v_words`.\n",
    "   3. Print the final size of the Brown corpus vocabulary.\n",
    "   4. Print the set of words in the Brown corpus vocabulary that are not in `w2v_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Construct word embeddings via PMI matrix factorization [30 points]\n",
    "\n",
    "Refer to the recitation of January 26, 2018 for hints (in case you do not have your notes handy, at the end of this problem's instructions, we have links to the recitation notes).\n",
    "\n",
    "   1. Construct a matrix `PMI` where each cell contains the pointwise mutual information between a pair of words in the preprocessed Brown corpus. Bigrams are counted for any two words occurring in the same sentence. (20 points)\n",
    "   2. Factorize `PMI` using SVD (`scipy.sparse.linalg.svds`) to obtain 100-dimensional vectors for each word, stored in a matrix `U`. Normalize each vector by its L2 norm. (10 points)\n",
    "   \n",
    "The PMI value for a pair of words (x, y) is (note that the recitation has a typo in this formula): $$ \\textrm{PMI}(x,y) = \\textrm{log}(\\frac{p(x,y)}{p(x)p(y)}) $$ where $p(x,y)$ is the probability of the bigram $x y$ occurring in the corpus, and $p(x)$ is the probability of the unigram $x$ occurring in the corpus.\n",
    "\n",
    "Be careful during division: dividing two integers may result in a 0, if none of them are first converted to `float`s.\n",
    "\n",
    "**Notes from the recitation on January 26, 2018:**\n",
    "\n",
    "* PDF: http://www.eyeshalfclosed.com/teaching/95865-recitation-word2vec_as_PMI.pdf\n",
    "* Jupyter notebook: https://gist.github.com/emaadmanzoor/1d06e0751a3f7d39bc6814941b37531d\n",
    "* Dataset: https://www.kaggle.com/hacker-news/hacker-news-posts/downloads/HN_posts_year_to_Sep_26_2016.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be used to test whether your normalization is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert np.allclose(np.array([np.linalg.norm(v) for v in U]),\n",
    "                   np.ones(U.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Evaluate how well the this method does on predicting word analogies [20 points]\n",
    "\n",
    "Please repeat part (b) from Method 1 except now using the word embedding stored in variable `U` computed for this second PMI-based method. Once again, print the total number of successes. **Then clearly state which method has a higher number of successes in predicting word analogies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer for which method does better** (just say either Method 1 or Method 2):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Briefly explain why we cannot easily do k-fold cross validation with either of the methods here. [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
