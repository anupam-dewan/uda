{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 95-865: Model Validation\n",
    "\n",
    "Author: George H. Chen (georgechen [at symbol] cmu.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important things you should get out of this class is how to assess how good different prediction methods are at a particular prediction task. Strangely enough, we do this by solving another prediction problem: for a given method, we predict how well it should do on test data that you don't get access to ahead of time!\n",
    "\n",
    "For example, for two methods A and B, we can predict their errors on the test set, and whichever one has lower *predicted* error is the one that we choose to use.\n",
    "\n",
    "Using this same idea, for a method with hyperparameters, we can try various values of the hyperparameter (you can think of it as if we are choosing between, say, 5 different values that the hyperparameter can be, then it's like choosing between 5 methods!). For each hyperparameter setting that we try, we can predict the error the method would have on the test set. Then we go with whichever hyperparameter setting resulted in the lowest *predicted* error.\n",
    "\n",
    "Note that the *predicted* error might not accurately correspond to the true error on the test set! (For example, I mentioned in lecture that if you're trying to choose between substantially more methods than there are training data, you are very likely to \"overfit\" the training data, meaning that you will find a method that seems to have an unrealistically low *predicted* error that dramatically underestimates the true error, and you end up choosing a lousy method that does not generalize well to the actual unseen test data!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with some usual boilerplate importing for setting up inline plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# the lines below are just for aesthetics\n",
    "plt.style.use('ggplot')  # if you want your plot to look at ggplot (like how R makes plots)\n",
    "%config InlineBackend.figure_format = 'retina'  # if you use a Mac with Retina display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load in the MNIST handwritten digit dataset. This dataset consists of 60,000 labeled images to train on (the labels are the true digits the images correspond to, e.g., 0, 1, 2, ..., 9), and 10,000 labeled images to test on. Each image is stored as a 28-by-28 numpy array (correspond to 28-by-28 pixel images), where each value is between 0 and 255 (0 being background, and 255 being foreground, but there are shades in between).\n",
    "\n",
    "**Warning:** The MNIST dataset neatly specifies what images are training data and which images are test data. In real-world situations, often times you will *not* be told what to use as training vs test data, and you are instead just given a single labeled dataset. In this case, *you* have to decide how to split up the data into train and test. There is no universal one-size-fits-all answer for how to do the splitting. Depending on the kind of data you are working with, it could even be that you only need 1% of the data to use as training data, whereas in many other cases, you might need something more like 70-80% of the data to use as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 7s 1us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the sizes of the train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take a look at what an example image looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x262d7cf6f60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x262d46c1ba8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, since the datasets are a little bit for the purposes of this demo, I'm going to take a subsample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images[:2000]\n",
    "train_labels = train_labels[:2000]\n",
    "test_images = test_images[:500]\n",
    "test_labels = test_labels[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, note that each image is stored as a 28-by-28 array of numbers. We want to flatten this out into a single-dimensional array, meaning a row vector that has 28 * 28 = 784 values. We can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 784)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_train_images = train_images.reshape(len(train_images), -1)  # flattens out each training image\n",
    "flattened_test_images = test_images.reshape(len(test_images), -1)  # flattens out each test image\n",
    "flattened_test_images.shape\n",
    "####Reshape with second parameter -1 indicates that the 28*28 image is flattened into 784 values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we normalize the images so that the values are between 0 and 1 instead of 0 and 255. Many machine learning methods can work better if numbers are close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalize\n",
    "flattened_train_images = flattened_train_images.astype(np.float32) / 255  # rescale to be between 0 and 1\n",
    "flattened_test_images = flattened_test_images.astype(np.float32) / 255  # rescale to be between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(flattened_train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 784)\n"
     ]
    }
   ],
   "source": [
    "print(flattened_test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first show how to use a $C$-SVM classifier using sklearn (I choose $C=1$ for now). For simplicity I fit on all the training data. I then predict on the *training* data (we want to eventually look at the test data, but following good practice when it comes to data, **you should not look at the test data until towards the end of your analysis**).\n",
    "\n",
    "You can sort of think of this setup as follows: the test set is a take-home exam with an answer key (that you're not supposed to look at, but it's actually there for you, with the idea that you self-grade). The training set is a collection of practice exams with answer keys. You can do whatever you want with the training set to prepare, and to guess how well you will do on the test set, but you should definitely not use anything from the test data to help your preparation! (That would be cheating!)\n",
    "\n",
    "To use a classifier in sklearn is like everything else in sklearn: you import the class you want to use, create an instance of it (and specify parameters), and then call `fit`, where now when we do the fitting, we have to specify both the feature vectors and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(C=1)\n",
    "classifier.fit(flattened_train_images, train_labels)\n",
    "###\n",
    "###classifier.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_train_labels = classifier.predict(flattened_train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 2, 0], dtype=uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.092\n"
     ]
    }
   ],
   "source": [
    "error_rate = np.mean(predicted_train_labels != train_labels)\n",
    "print(error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing hyperparameter $C$ using simple data splitting\n",
    "\n",
    "In the code above, I set $C=1$. How do we choose $C$? The simplest way is to just divide up the training data into a smaller training set and a validation set. Put another way, we're just splitting up the data so that now some of it (namely the validation set) is being treated as \"test\" data even though it's not actually the test data. We train on the new smaller training set and test on the validation set. We repeat this for different values of $C$ and pick whichever value of $C$ results in the lowest error on the validation set. Effectively we are using the error on the validation set to predict what the error might be for the true test set that we don't get to see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "[ 509 1801  579 ...,  427 1488 1070]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(95865)  # make randomness deterministic (otherwise re-running the notebook will give different results)\n",
    "num_train_images = len(flattened_train_images)\n",
    "print(num_train_images)##2000\n",
    "shuffled_indices = np.random.permutation(num_train_images)\n",
    "print(shuffled_indices)#2000 indices shuffled up\n",
    "\n",
    "train_frac = 0.7 ## Splitting fraction generally 0.7-0.75\n",
    "smaller_train_indices = shuffled_indices[:int(train_frac*num_train_images)] ###pick inices on 0th point to 0.7*2000=1400th indice\n",
    "validation_indices = shuffled_indices[int(train_frac*num_train_images):]#Give Validation set from 1400 to  last  inidice in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.01 error rate: 0.906666666667\n",
      "C: 0.1 error rate: 0.695\n",
      "C: 1 error rate: 0.128333333333\n",
      "C: 10 error rate: 0.0983333333333\n",
      "C: 100 error rate: 0.105\n",
      "C: 1000 error rate: 0.103333333333\n",
      "Best C: 10 error rate: 0.0983333333333\n"
     ]
    }
   ],
   "source": [
    "lowest_error = np.inf\n",
    "best_C = None\n",
    "X_val=flattened_train_images[validation_indices]\n",
    "Y_val=train_labels[validation_indices]\n",
    "X_train_new=flattened_train_images[smaller_train_indices]\n",
    "Y_train_new=train_labels[smaller_train_indices]\n",
    "for C in [1e-2, 1e-1, 1, 10, 100, 1000]:\n",
    "    classifier = SVC(C=C)\n",
    "    classifier.fit(X_train_new,Y_train_new)\n",
    "    Y_val_pred = classifier.predict(X_val)\n",
    "    error = np.mean(Y_val_pred!=Y_val)\n",
    "    print('C:', C, 'error rate:', error)\n",
    "    \n",
    "    if error < lowest_error:\n",
    "        lowest_error = error\n",
    "        best_C = C\n",
    "\n",
    "print('Best C:', best_C, 'error rate:', lowest_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing hyperparameter $C$ using $k$-fold cross validation\n",
    "\n",
    "Simple data splitting is easy to do. However, the drawback is that depending on the validation set that is typically randomly chosen, we can get quite different predictions for what the validation error is for each hyperparameter choice. One strategy to combat this is to actually, for each hyperparameter choice, try many different validation sets rather than only 1. A popular way of choosing validation sets so that every training data point gets used at some point within a validation set once is $k$-fold cross validation. Per hyperparameter setting, we now use $k$ different validation sets corresponding to the folds (see lecture slides for how this works).\n",
    "\n",
    "The drawback of using cross validation compared to data splitting, which is immediately apparent in code, is that it's more expensive: we have another for loop over the cross validation folds, and so the amount of models we need to fit gets multiplied by the number of folds. This can be extremely expensive if model fitting is costly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- C: 0.01 FOLD 1 error rate: 0.915\n",
      "------- C: 0.01 FOLD 2 error rate: 0.91\n",
      "------- C: 0.01 FOLD 3 error rate: 0.9\n",
      "------- C: 0.01 FOLD 4 error rate: 0.8925\n",
      "------- C: 0.01 FOLD 5 error rate: 0.9125\n",
      "C: 0.01 cross validation error: 0.906\n",
      "------- C: 0.1 FOLD 1 error rate: 0.585\n",
      "------- C: 0.1 FOLD 2 error rate: 0.5625\n",
      "------- C: 0.1 FOLD 3 error rate: 0.6125\n",
      "------- C: 0.1 FOLD 4 error rate: 0.595\n",
      "------- C: 0.1 FOLD 5 error rate: 0.575\n",
      "C: 0.1 cross validation error: 0.586\n",
      "------- C: 1 FOLD 1 error rate: 0.11\n",
      "------- C: 1 FOLD 2 error rate: 0.1025\n",
      "------- C: 1 FOLD 3 error rate: 0.1125\n",
      "------- C: 1 FOLD 4 error rate: 0.1425\n",
      "------- C: 1 FOLD 5 error rate: 0.115\n",
      "C: 1 cross validation error: 0.1165\n",
      "------- C: 10 FOLD 1 error rate: 0.0825\n",
      "------- C: 10 FOLD 2 error rate: 0.065\n",
      "------- C: 10 FOLD 3 error rate: 0.085\n",
      "------- C: 10 FOLD 4 error rate: 0.095\n",
      "------- C: 10 FOLD 5 error rate: 0.095\n",
      "C: 10 cross validation error: 0.0845\n",
      "------- C: 100 FOLD 1 error rate: 0.0775\n",
      "------- C: 100 FOLD 2 error rate: 0.0925\n",
      "------- C: 100 FOLD 3 error rate: 0.09\n",
      "------- C: 100 FOLD 4 error rate: 0.1\n",
      "------- C: 100 FOLD 5 error rate: 0.1\n",
      "C: 100 cross validation error: 0.092\n",
      "------- C: 1000 FOLD 1 error rate: 0.075\n",
      "------- C: 1000 FOLD 2 error rate: 0.09\n",
      "------- C: 1000 FOLD 3 error rate: 0.0875\n",
      "------- C: 1000 FOLD 4 error rate: 0.1\n",
      "------- C: 1000 FOLD 5 error rate: 0.1025\n",
      "C: 1000 cross validation error: 0.091\n",
      "Best C: 10 cross validation error: 0.0845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "lowest_cross_val_error = np.inf\n",
    "best_C = None\n",
    "\n",
    "indices = range(num_train_images)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=95865)\n",
    "for C in [1e-2, 1e-1, 1, 10, 100, 1000]:\n",
    "    errors = []\n",
    "    ###kf.split(indices): Splits indices into 5 portions randomly \n",
    "    ####one by one keeping one as val set and rest 4 as train set\n",
    "    ###each time and does it 5 times for each C so 5 C vals and for each C 5 folds so 25 times we perform CV\n",
    "    #### sO  \n",
    "    fold_num=0\n",
    "    for train_indices, val_indices in kf.split(indices):\n",
    "        fold_num=fold_num+1\n",
    "        X_val=flattened_train_images[val_indices]\n",
    "        Y_val=train_labels[val_indices]\n",
    "        X_train_new=flattened_train_images[train_indices]\n",
    "        Y_train_new=train_labels[train_indices]\n",
    "        classifier = SVC(C=C)\n",
    "        classifier.fit(X_train_new,Y_train_new)\n",
    "        Y_val_pred = classifier.predict(X_val)\n",
    "        error = np.mean(Y_val_pred!=Y_val)\n",
    "        print('------- C:', C,'FOLD',fold_num ,'error rate:', error)\n",
    "        errors.append(error)\n",
    "    cross_val_error = np.mean(errors) ### Cross val error is mean of all 5 errors obtained from 5 different fold this is cross val error for that paticular C so we get such 5 cross val errors\n",
    "    print('C:', C, 'cross validation error:', cross_val_error)\n",
    "\n",
    "    if cross_val_error < lowest_cross_val_error:\n",
    "        lowest_cross_val_error = cross_val_error\n",
    "        best_C = C\n",
    "\n",
    "print('Best C:', best_C, 'cross validation error:', lowest_cross_val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using different classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code works if we swap out the $C$-SVM classifier for a different classifier that has some different hyperparameter(s). For example, see if you can figure out how to use data splitting and 5-fold cross validation to choose the number of trees to use with a random forest. I show below how to train a random forest and then I predict on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=50, random_state=95865)  # n_estimators is the number of trees\n",
    "rf_classifier.fit(flattened_train_images, train_labels)\n",
    "rf_predicted_train_labels = rf_classifier.predict(flattened_train_images)\n",
    "rf_error = np.mean(rf_predicted_train_labels != train_labels)\n",
    "print(rf_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarkable: a training error of 0! Of course, this is clearly an overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally actually looking at the test data\n",
    "\n",
    "Once you've done everything you want to do with the training data (namely trying whatever prediction methods you want to try, and for each one picking the best parameters using, say, cross validation), you can then use each method (with the best parameters from cross validation on training data) to predict on the test data and report the method's  error on the test set. We show how to do this for $C$-SVM using the best value of $C$ determined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "final_svm_classifier = SVC(C=best_C)\n",
    "final_svm_classifier.fit(flattened_train_images, train_labels)\n",
    "predicted_test_labels = final_svm_classifier.predict(flattened_test_images)\n",
    "test_set_error = np.mean(predicted_test_labels != test_labels)\n",
    "print(test_set_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in general the cross validation error is not going to perfectly match up with the test set error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
